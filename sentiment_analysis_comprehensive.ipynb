{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive Sentiment Analysis with Deep Learning Models\\n",
        "\\n",
        "This notebook provides a complete, self-contained implementation of sentiment analysis using various deep learning architectures. The implementations are based on insights from key research papers and run sequentially without external dependencies except for the CSV dataset created in this notebook.\\n",
        "\\n",
        "## Literature Review and Research Paper Applications\\n",
        "\\n",
        "### 1. \\\"Attention Is All You Need\\\" by Vaswani et al. (2017)\\n",
        "**Key Contribution**: Introduced the Transformer architecture using self-attention mechanisms instead of recurrence.\\n",
        "**Our Implementation**: The TransformerModel class implements multi-head self-attention and positional encodings. We use this architecture for capturing long-range dependencies more effectively than RNNs, directly applying the paper's insight that self-attention allows models to focus on relevant parts of the input sequence.\\n",
        "\\n",
        "### 2. \\\"Bidirectional LSTM-CRF Models for Sequence Tagging\\\" by Huang, Xu, and Yu (2015)\\n",
        "**Key Contribution**: Demonstrated the power of bidirectional processing for sequence understanding.\\n",
        "**Our Implementation**: Our BidirectionalLSTMModel and BidirectionalGRUModel process sequences in both directions. This is crucial for sentiment analysis where future context affects meaning (e.g., \\\"The movie was not bad at all\\\" - the sentiment depends on words that come after \\\"not bad\\\").\\n",
        "\\n",
        "### 3. \\\"A Structured Self-Attentive Sentence Embedding\\\" by Lin et al. (2017)\\n",
        "**Key Contribution**: Introduced self-attention for creating interpretable sentence embeddings.\\n",
        "**Our Implementation**: Our LSTMWithAttentionModel and GRUWithAttentionModel implement this approach, using attention weights over all hidden states instead of just the final output. This creates more informative sentence representations by focusing on the most relevant words.\\n",
        "\\n",
        "### 4. \\\"GloVe: Global Vectors for Word Representation\\\" by Pennington, Socher, and Manning (2014)\\n",
        "**Key Contribution**: Demonstrated that pre-trained embeddings capture semantic relationships through global co-occurrence statistics.\\n",
        "**Our Implementation**: While we use randomly initialized embeddings for self-containment, this paper provides the theoretical foundation for why embedding layers are so crucial and could be enhanced with pre-trained vectors.\\n",
        "\\n",
        "### 5. \\\"Bag of Tricks for Efficient Text Classification\\\" by Joulin et al. (2016)\\n",
        "**Key Contribution**: Showed that simple models can be surprisingly effective for text classification.\\n",
        "**Our Implementation**: This paper guides our inclusion of simple baseline models and efficient tokenization, serving as sanity checks against more complex architectures.\\n",
        "\\n",
        "## Notebook Structure\\n",
        "1. **Environment Setup**: Dependencies and configuration\\n",
        "2. **Data Collection**: Synthetic dataset generation (GetData cell)\\n",
        "3. **Utilities**: Text processing and tokenization functions\\n",
        "4. **Model Implementations**: All neural network architectures\\n",
        "5. **Training Framework**: Training loops with advanced techniques\\n",
        "6. **Evaluation System**: Comprehensive metrics and analysis\\n",
        "7. **Hyperparameter Tuning**: Systematic optimization\\n",
        "8. **Model Comparison**: Head-to-head performance analysis\\n",
        "9. **Results Visualization**: Performance plots and analysis\\n",
        "10. **Conclusions**: Summary and recommendations\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\\n",
        "import sys\\n",
        "import time\\n",
        "import warnings\\n",
        "import numpy as np\\n",
        "import pandas as pd\\n",
        "import matplotlib.pyplot as plt\\n",
        "import seaborn as sns\\n",
        "from collections import Counter\\n",
        "import math\\n",
        "import random\\n",
        "\\n",
        "import torch\\n",
        "import torch.nn as nn\\n",
        "import torch.optim as optim\\n",
        "import torch.nn.functional as F\\n",
        "from torch.utils.data import DataLoader, TensorDataset\\n",
        "\\n",
        "from sklearn.model_selection import train_test_split\\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\\n",
        "\\n",
        "# Set random seeds for reproducibility\\n",
        "torch.manual_seed(42)\\n",
        "np.random.seed(42)\\n",
        "random.seed(42)\\n",
        "if torch.cuda.is_available():\\n",
        "    torch.cuda.manual_seed(42)\\n",
        "\\n",
        "warnings.filterwarnings('ignore')\\n",
        "plt.style.use('default')\\n",
        "\\n",
        "device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n",
        "print(f\\\"Using device: {device}\\\")\\n",
        "print(f\\\"PyTorch version: {torch.__version__}\\\")\\n",
        "\\n",
        "os.makedirs('models', exist_ok=True)\\n",
        "os.makedirs('results', exist_ok=True)\\n",
        "print(\\\"Environment setup complete!\\\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}